\documentclass[a4paper,11pt]{article}
\usepackage{filecontents}

\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{graphicx, array, blindtext}
\usepackage[colorinlistoftodos]{todonotes}
\DeclareUnicodeCharacter{2212}{-}
\usepackage [a4 paper , hmargin = 1.2 in , bottom = 1.5 in] {geometry}
\usepackage [parfill] {parskip}

\usepackage{enumitem}
\usepackage{amsmath}
\usepackage{amsthm}

\usepackage{nameref}
\usepackage{amssymb}
\usepackage [linesnumbered, ruled, vlined] {algorithm2e}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{floatrow}
\usepackage{siunitx}
\usepackage{cancel}
\usepackage{fancyhdr}
\usepackage{graphicx}
\usepackage{verbatim}
\usepackage[document]{ragged2e}

\renewcommand{\footrulewidth}{0.4pt}
\newtheorem{definition}{Definition}
\numberwithin{definition}{section}
\newtheorem{mytheorem}{Theorem}
\numberwithin{mytheorem}{subsection}
\newcommand{\notimplies}{\;\not\!\!\!\longrightarrow}  
\newcommand\norm[1]{\left\lVert#1\right\rVert}
\pagestyle{fancy}
\fancyhf{}
\rhead{CS754 Assignment 3}
\lhead{200050013-200050130}
\fancyfoot[C]{Page \thepage}
\usepackage{subcaption}
\usepackage{listings}


\usepackage{hyperref}
\urlstyle{same}
\hypersetup{pdftitle={main.pdf},
    colorlinks=false,
    linkbordercolor=red
}
\usepackage{array}
\usepackage{listings,chngcntr}

\begin{document}
\centering{

\title{\fontsize{150}{60}{CS754 Assignment 3 Report}}

\author{
Arpon Basu \\ Shashwat Garg }
}

\date{Spring 2022}
\maketitle

\justifying
\tableofcontents

\newpage
\justifying
\section*{Introduction}

Welcome  to our report on CS754 Assignment 3. We have tried to make this report comprehensive and self-contained. We hope reading this would give you a proper flowing description of our work, methods used and the results obtained.

Hope you enjoy reading the report. Here we go!


\section{Problem 1}


\section{Problem 2}
(a) The restricted eigenvalue property has a very neat and succinct description in terms of the SVD of our sensing matrix $\boldsymbol{X}$, and that's how we'll define it.\\
Note that the restricted eigenvalue property for the matrix $X$ is defined as follows
$$\frac{1}{N\cdot \lVert\nu\rVert_2^2}\nu^TX^TX\nu \geq \gamma \;\forall\;\nu \in \mathbb{K}^{p\times 1}\setminus\{\mathbf{0}\}$$
where $\gamma$ is the coefficient of the property.\\
Now, notice that if we restrict $\nu$ to vary over only unit length vectors in $\mathbb{K}^{p\times 1}$, then we can get rid of the $\lVert\nu\rVert_2^2$ in the denominator. Also, since the inequality holds for all $\nu$ in our chosen space, it's enough to assume that the infimum of the expression $\frac{1}{N}\nu^TX^TX\nu$ over all $\nu$ such that $\lVert\nu\rVert_2 = 1$ must be greater than or equal to $\gamma$, ie:- 
$$\mathrm{min}_{\lVert\nu\rVert_2 = 1}\frac{1}{N}\nu^TX^TX\nu \geq \gamma$$
$$\implies \mathrm{min}_{\lVert\nu\rVert_2 = 1}(X\nu)^TX\nu \geq N\gamma$$
$$\implies \mathrm{min}_{\lVert\nu\rVert_2 = 1}\lVert X\nu\rVert_2^2 \geq N\gamma$$
$$\implies \mathrm{min}_{\lVert\nu\rVert_2 = 1}\lVert X\nu\rVert_2 \geq \sqrt{N\gamma}$$
But from the theory of Singular Value Decomposition, we know that $\mathrm{min}_{\lVert\nu\rVert_2 = 1}\lVert X\nu\rVert_2$ is equal to the smallest singular value of $X$.\\
Thus, we can \textbf{define} the \textbf{Restricted Eigenvalue Property} with coefficient $\gamma$ as follows : The matrix $\boldsymbol{X}\in\mathbb{R}^{n\times p}$ is said to satisfy the Restricted Eigenvalue Property with coefficient $\gamma$ if the \textbf{smallest singular value of $\boldsymbol{X}$ is atleast $\sqrt{N\gamma}$}.\\
(b) We have that 
$$G(\nu) := \frac{1}{2N}\lVert y - X(\beta^{*} + \nu)\rVert^2_2 + \lambda_N\lVert\beta^{*} + \nu\rVert_1$$
$$J(\boldsymbol{x}) := \frac{1}{2N}\lVert y - X\boldsymbol{x}\rVert^2_2 + \lambda_N\lVert\boldsymbol{x}\rVert_1$$
Thus 
$$G(0) := \frac{1}{2N}\lVert y - X\beta^{*}\rVert^2_2 + \lambda_N\lVert\beta^{*}\rVert_1 = J(\beta^{*})$$
On the other hand, for $\hat{\nu} := \hat{\beta} - \beta^{*}$, we have
$$G(\hat{\nu}) := \frac{1}{2N}\lVert y - X\hat{\beta}\rVert^2_2 + \lambda_N\lVert\hat{\beta}\rVert_1 = J(\hat{\beta})$$
But $\hat{\beta}$ was, \textbf{by construction} (as mentioned on Pg\# 308, third last line), the optimizer of the LASSO function $J(\boldsymbol{x})$, and consequently $G(\hat{\nu}) = J(\hat{\beta}) \leq J(\beta^*) = G(0)$, as desired.\\
(c) We just have to piece together some equations and inequalities to derive equation 11.21. They go as follows:
$$G(\hat{\nu}) \leq G(0)$$
$$y = X\beta^* + w$$
$$\hat{\beta} = \beta^* + \hat{\nu}$$
The first inequality $G(\hat{\nu}) \leq G(0)$, combined with the fact that $\hat{\beta} = \beta^* + \hat{\nu}$ yields
$$G(\hat{\nu}) \leq G(0)$$
$$\implies \frac{1}{2N}\lVert y - X\hat{\beta}\rVert^2_2 + \lambda_N\lVert\hat{\beta}\rVert_1 \leq \frac{1}{2N}\lVert y - X\beta^{*}\rVert^2_2 + \lambda_N\lVert\beta^{*}\rVert_1$$
$$\implies \frac{1}{2N}\lVert y - X(\beta^* + \hat{\nu})\rVert^2_2 + \lambda_N\lVert\beta^* + \hat{\nu}\rVert_1 \leq \frac{1}{2N}\lVert y - X\beta^{*}\rVert^2_2 + \lambda_N\lVert\beta^{*}\rVert_1$$
Now, using the fact that $y = X\beta^* + w$, we get
$$\frac{1}{2N}\lVert y - X(\beta^* + \hat{\nu})\rVert^2_2 + \lambda_N\lVert\beta^* + \hat{\nu}\rVert_1 \leq \frac{1}{2N}\lVert y - X\beta^{*}\rVert^2_2 + \lambda_N\lVert\beta^{*}\rVert_1$$
$$\implies \frac{1}{2N}\lVert y - X\beta^* - X\hat{\nu}\rVert^2_2 + \lambda_N\lVert\beta^* + \hat{\nu}\rVert_1 \leq \frac{1}{2N}\lVert w\rVert^2_2 + \lambda_N\lVert\beta^{*}\rVert_1$$
$$\implies \frac{1}{2N}\lVert w - X\hat{\nu}\rVert^2_2 \leq \frac{1}{2N}\lVert w\rVert^2_2 + \lambda_N(\lVert\beta^{*}\rVert_1 - \lVert\beta^* + \hat{\nu}\rVert_1)$$
Now, $\lVert w - X\hat{\nu}\rVert^2_2 = (w - X\hat{\nu})^T\cdot(w - X\hat{\nu}) = \lVert w\rVert^2_2 + \lVert X\hat{\nu}\rVert^2_2 - 2w^TX\hat{\nu} $. Substituting this in the equation above yields
$$\frac{1}{2N}(\lVert w\rVert^2_2 + \lVert X\hat{\nu}\rVert^2_2 - 2w^TX\hat{\nu}) \leq \frac{1}{2N}\lVert w\rVert^2_2 + \lambda_N\{\lVert\beta^{*}\rVert_1 - \lVert\beta^* + \hat{\nu}\rVert_1\}$$
$$\implies \frac{1}{2N}(\lVert X\hat{\nu}\rVert^2_2 - 2w^TX\hat{\nu}) \leq \lambda_N\{\lVert\beta^{*}\rVert_1 - \lVert\beta^* + \hat{\nu}\rVert_1\}$$
$$\implies \frac{1}{2N}\lVert X\hat{\nu}\rVert^2_2 - \frac{1}{N}w^TX\hat{\nu}\leq \lambda_N\{\lVert\beta^{*}\rVert_1 - \lVert\beta^* + \hat{\nu}\rVert_1\}$$
$$\implies \frac{\lVert X\hat{\nu}\rVert^2_2}{2N} \leq \frac{w^TX\hat{\nu}}{N} + \lambda_N\{\lVert\beta^{*}\rVert_1 - \lVert\beta^* + \hat{\nu}\rVert_1\}$$
as desired.\\
(d) We use a version of H\"{o}lder's inequality for vectors as follows:\\
Let $p, q\in[1,\infty]$ be two real numbers such that $\frac{1}{p} + \frac{1}{q} = 1$. Let $\boldsymbol{x} = (x_1, x_2, ..., x_n)$ and $\boldsymbol{y} = (y_1, y_2, ..., y_n)$ be two vectors. Then
$$\boldsymbol{\sum^{n}_{k = 1}|x_ky_k|\leq (\sum^{n}_{k = 1}|x_k|^p)^{\frac{1}{p}}(\sum^{n}_{k = 1}|y_k|^q)^{\frac{1}{q}} = \lVert x\rVert_p\lVert y\rVert_q}$$
We augment this inequality for our purposes: Note that $\boldsymbol{x}\cdot\boldsymbol{y} = \sum^{n}_{k = 1}x_ky_k \leq \sum^{n}_{k = 1}|x_ky_k| \leq (\sum^{n}_{k = 1}|x_k|^p)^{\frac{1}{p}}(\sum^{n}_{k = 1}|y_k|^q)^{\frac{1}{q}}$, thus showing that $\boldsymbol{x}\cdot\boldsymbol{y}\leq (\sum^{n}_{k = 1}|x_k|^p)^{\frac{1}{p}}(\sum^{n}_{k = 1}|y_k|^q)^{\frac{1}{q}} = \lVert \boldsymbol{x}\rVert_p\lVert \boldsymbol{y}\rVert_q$.\\
In our context let $\boldsymbol{x} = X^Tw$ and $\boldsymbol{y} = \nu$, and let $p =\infty$, $q = 1$. Then, $(X^Tw)\cdot\nu = (X^Tw)^T\nu = w^TX\nu$. Then we have that 
$$(X^Tw)\cdot\nu\leq \lVert X^Tw\rVert_{\infty}\lVert \nu\rVert_1$$
$$\implies w^TX\nu\leq \lVert X^Tw\rVert_{\infty}\lVert \nu\rVert_1$$
Thus, 
$$\frac{w^TX\hat{\nu}}{N} + \lambda_N\{\lVert\hat{\nu_S}\rVert_1 - \lVert\hat{\nu_{S^c}}\rVert_1\}\leq \frac{\lVert X^Tw\rVert_{\infty}}{N}\lVert \nu\rVert_1 + \lambda_N\{\lVert\hat{\nu_S}\rVert_1 - \lVert\hat{\nu_{S^c}}\rVert_1\}$$
as desired.\\
% % (e) How much to write here?\\
% (f)\\
% % (g) When we show $\frac{\lVert X\hat{\nu}\rVert^2_2}{2N}\leq \frac{3}{2}\sqrt{k}\lambda_N\lVert\hat{\nu}\rVert_2^2$, we need to use the fact $\frac{1}{N}\lVert X^Tw\rVert_{\infty}\leq\frac{\lambda_N}{2}$. It's here that we need to utilise the fact that $\lambda_N \geq 2\frac{\lVert X^Tw\rVert_{\infty}}{N}$\\
% (h)\\
% (i)\\
% (j)

\section{Problem 3}


\section{Problem 4}


\begin{center}
    \begin{tabular}{ |p{3.5cm}||p{10cm}|}
   
    \hline
    \multicolumn{2}{|c|}{Paper Details} \\
    \hline
    Title of the Paper& Coastal Acoustic Tomography System and 
    
    Its Field Application\\
    \hline
    Link of the paper  &  \href{https://ieeexplore.ieee.org/document/1002483}{\textbf{Click Here}}  \\
    \hline
    Author List & Haruhiko Yamoaka, Arata Kaneko, Jae-Hun Park, Hong Zheng, Noriaki Gohda, Tadashi Takano, Xiao-Hua Zhu and Yoshio Takasugi \\
    \hline
    Publication Date  & August 2002 \\
    \hline
    Publication Venue  &  IEEE Journal of Oceanic Engineering, Volume 27, Issue 2 \\
    \hline
   \end{tabular}
\end{center}

\subsection{Introduction and Aim}

This paper aims to map the structure of the ``strongly
nonlinear tidal currents in the coastal sea'' by using multiple synchronised coastal acoustic tomography system (CATS). Using GPS clock signals and separate codes to distinguish between signals of individual systems, reconstruction of tidal process behaviour is done through an inverse analysis of the acoustic signals obtained by the sensors.

\subsection{Mathematical Formulation}



\section{Problem 5}

We know that Radon Transform is given by-
$$R_\theta(f) =g(\rho, \theta)= \int_{-\infty}^{+\infty}f(\rho\cos\theta - z\sin\theta,\rho\sin \theta + z\cos\theta)dz $$
We can write the same as-
$$R_\theta(f) =g(\rho, \theta)= \int_{-\infty}^{+\infty}\int_{-\infty}^{+\infty}f(x,y)\delta(x\cos\theta+y\sin\theta -\rho)dxdy $$
Let the scaled image be denoted by $h(x,y) = f(ax, ay)$. This is the same image as original, but scaled by a factor of a, in both x and y directions.

We can write the same Radon Transform as-
$$ R_\theta(h) =g^{\prime}(\rho, \theta)= \int_{-\infty}^{+\infty}\int_{-\infty}^{+\infty}h(x,y)\delta(x\cos\theta+y\sin\theta -\rho)dxdy $$
$$ R_\theta(h) =g^{\prime}(\rho, \theta)= \int_{-\infty}^{+\infty}\int_{-\infty}^{+\infty}f(ax,ay)\delta(x\cos\theta+y\sin\theta -\rho)dxdy $$
$$ R_\theta(h) =g^{\prime}(\rho, \theta)= \int_{-\infty}^{+\infty}\int_{-\infty}^{+\infty}f(x^{\prime},y^{\prime})\delta\bigg(\frac{x^{\prime}\cos\theta+y^{\prime}\sin\theta -a\rho}{a}\bigg)\frac{dx^{\prime}}{a}\frac{dy^{\prime}}{a} $$

Since $\delta(ax) = \delta(x)/a$, we get-
$$ R_\theta(h) =g^{\prime}(\rho, \theta)= \frac{1}{a}\int_{-\infty}^{+\infty}\int_{-\infty}^{+\infty}f(x^{\prime},y^{\prime})\delta(x^{\prime}\cos\theta+y^{\prime}\sin\theta -a\rho)dx^{\prime}dy^{\prime} $$
$$ R_\theta(h) =g^{\prime}(\rho, \theta)= \frac{1}{a}g(a\rho, \theta) $$

Thus, we can see that the Radon transform of the scaled image is also scaled by a factor of $a$ in the size of projection, but the intensity of each projection has reduced by $a$ as well.
 








\section{Problem 6}

We know that the Radon Transform is given by-
$$R_\theta(f) =g(\rho, \theta)= \int_{-\infty}^{+\infty}f(\rho \cos\theta - z\sin\theta,\rho \sin \theta + z \cos\theta)dz $$
We can write the same as-
$$R_\theta(f) =g(\rho, \theta)= \int_{-\infty}^{+\infty}\int_{-\infty}^{+\infty}f(x,y)\delta(x\cos\theta+y\sin\theta -\rho)dxdy $$
Now, let $f(x, y) = \delta(x-x_0, y-y_0)$ for some given constants $x_0$, $y_0$. Also, for the sake of simplification, call $\delta(x\cos\theta+y\sin\theta -\rho)$ as $h(x, y, \rho, \theta)$.\\
Then the Radon transform of our function $f(x,y)$ becomes
$$\int_{-\infty}^{+\infty}\int_{-\infty}^{+\infty}f(x,y)\delta(x\cos\theta+y\sin\theta -\rho)dxdy$$
$$ = \int_{-\infty}^{+\infty}\int_{-\infty}^{+\infty}\delta(x-x_0, y-y_0)h(x,y,\rho,\theta)dxdy$$
Now, by a well known property of delta functions, the integration of a function multiplied by the delta function over any space (including the delta function's singularity) yields the evaluation of the function at the singularity point. In the context of our problem, we can state the above property as 
$$\int_{-\infty}^{+\infty}\int_{-\infty}^{+\infty}\delta(x-x_0, y-y_0)h(x,y)dxdy = h(x_0, y_0)$$
Applying this property verbatim on our integral above yields
$$\int_{-\infty}^{+\infty}\int_{-\infty}^{+\infty}\delta(x-x_0, y-y_0)h(x,y,\rho,\theta)dxdy$$
$$ = h(x_0, y_0, \rho, \theta) = \delta(x_0\cos\theta+y_0\sin\theta -\rho)$$
since $\rho$, $\theta$ are constant parameters within the integration.\\
Thus the Radon transform of the unit impulse function is another impulse function, ie:-
$$\boldsymbol{R_{\theta}(\delta(x-x_0, y-y_0))) = g(\rho, \theta) = \delta(x_0\cos\theta+y_0\sin\theta -\rho)}$$
$$\boldsymbol{R_{\theta}(\delta(x, y))) = g(\rho, \theta) = \delta(-\rho) = \delta(\rho)}$$








\end{document}

